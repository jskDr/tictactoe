{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman for TicTacToe with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas based Bellman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### v(s)\n",
      "v[0]=-2.5057415905523253\n",
      "v[1]=-1.6060272426565467\n",
      "v[2]=2.4316473139847745\n",
      "v[3]=7.135350237940245\n",
      "v[4]=0.0\n",
      "### q(s,a)\n",
      "q[0][0]=-3.3266059231766705\n",
      "q[0][1]=-1.6848772579279794\n",
      "q[1][0]=-3.3516633390821924\n",
      "q[1][1]=0.13960885376910126\n",
      "q[2][0]=0.0\n",
      "q[2][1]=4.863294627969549\n",
      "q[3][0]=4.270700475880492\n",
      "q[3][1]=10.0\n",
      "---------------------------------\n",
      "### v(s)\n",
      "v[0]=-2.5057415905523253\n",
      "v[1]=-1.6060272426565467\n",
      "v[2]=2.4316473139847745\n",
      "v[3]=7.135350237940245\n",
      "v[4]=0.0\n",
      "### q(s,a)\n",
      "q[0][0]=-3.3266059231766705\n",
      "q[0][1]=-1.6848772579279794\n",
      "q[1][0]=-3.3516633390821924\n",
      "q[1][1]=0.13960885376910126\n",
      "q[2][0]=0.0\n",
      "q[2][1]=4.863294627969549\n",
      "q[3][0]=4.270700475880492\n",
      "q[3][1]=10.0\n",
      "---------------------------------\n",
      "### v(s)\n",
      "v[0]=0.1666666666666665\n",
      "v[1]=0.0\n",
      "v[2]=0.0\n",
      "v[3]=0.0\n",
      "v[4]=0.0\n",
      "### q(s,a)\n",
      "q[0][0]=1.0\n",
      "q[0][1]=0.0\n",
      "q[0][2]=-0.5\n",
      "q[1][0]=0.0\n",
      "q[2][0]=0.0\n",
      "q[3][0]=0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "두 클라스를 공통인 MDP에서 상속하게 만듬. \n",
    "L은 list를 이용하는 방식이고 PD는 pandas를 이용하는 방식임.\n",
    "두 방식에 충돌이 나는 경우, 기반이 되는 MDP에는 공통 요소만 남기고 나머지는 MDP_L에 옮겨둘 예정임.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self):\n",
    "        self.init_policy_Rsa_Psas()\n",
    "        \n",
    "    def init_policy_Rsa_Psas(self):\n",
    "        # policy[state][action] = 0.9 (<-- prob)\n",
    "        # None state is the last state        \n",
    "        self.policy_actions_table = [['Facebook', 'Quit'], ['Facebook', 'Study'], \n",
    "            ['Sleep', 'Study'], ['Pub', 'Study'], None]\n",
    "        self.Rsa = [[-1,0], [-1,-2], [0, -2], [1,10]]\n",
    "        self.N_states = len(self.policy_actions_table)\n",
    "        self.N_actions_in_s = []\n",
    "        self.policy = []\n",
    "        for actions in self.policy_actions_table:\n",
    "            if actions:\n",
    "                N_actions = len(actions)\n",
    "                self.N_actions_in_s.append(N_actions)\n",
    "                self.policy.append(np.ones(N_actions)/N_actions)\n",
    "            else:\n",
    "                self.N_actions_in_s.append(0)\n",
    "                \n",
    "        # policy가 고려되지 않은 관계임. Policy에 따른 가중에 별도로 고려되어야 함.\n",
    "        self.Psas = np.zeros([self.N_states, 2, self.N_states]) # Probability\n",
    "        self.Psas[0,0,0], self.Psas[0,1,1] = 1.0, 1.0\n",
    "        self.Psas[1,0,0], self.Psas[1,1,2] = 1.0, 1.0\n",
    "        self.Psas[2,0,4], self.Psas[2,1,3] = 1.0, 1.0\n",
    "        self.Psas[3,0,1], self.Psas[3,0,2], \\\n",
    "            self.Psas[3,0,3], self.Psas[3,1,4] = 0.2, 0.4, 0.4, 1.0 \n",
    "        \n",
    "    def init_v(self):\n",
    "        self.v = np.zeros(self.N_states)\n",
    "        \n",
    "    def init_q(self):\n",
    "        self.q = []\n",
    "        for s in range(self.N_states-1):\n",
    "            self.q.append(np.zeros(self.N_actions_in_s[s]))\n",
    "        self.q.append(0)\n",
    "        #print('q=', self.q)\n",
    "    \n",
    "    def get_v(self, N_iter:int=10) -> np.ndarray:\n",
    "        self.init_v()\n",
    "        for n in range(N_iter):\n",
    "            for s in range(self.N_states-1):\n",
    "                self.v[s] = (self.v[s] * n + self.bellman_v(s))/(n+1)        \n",
    "        \n",
    "        for s in range(self.N_states):\n",
    "            print(f'v[{s}]={self.v[s]}')\n",
    "        return self.v\n",
    "    \n",
    "    def get_q(self, N_iter:int=10) -> List:\n",
    "        self.init_q()\n",
    "        \n",
    "        for n in range(N_iter):\n",
    "            for s in range(self.N_states-1):\n",
    "                for a in range(self.N_actions_in_s[s]): \n",
    "                    #print(f'[?]s,a={s,a} --> {self.q[s][a]}')\n",
    "                    self.q[s][a] = (self.q[s][a] * n + \n",
    "                        self.bellman_q(s,a))/(n+1)  \n",
    "                    #self.q[s][a] = (self.q[s][a] * n)/(n+1) \n",
    "        \n",
    "        for s in range(self.N_states-1):\n",
    "            for a in range(self.N_actions_in_s[s]):\n",
    "                print(f'q[{s}][{a}]={self.q[s][a]}')\n",
    "        return self.q      \n",
    "    \n",
    "    def bellman_v(self, s:int=0, forgetting_factor:float=1.0) -> float:\n",
    "        Gs = 0\n",
    "        for a in range(len(self.policy[s])):\n",
    "            Gs += self.policy[s][a] * self.bellman_q_by_v(\n",
    "                s=s, a=a, forgetting_factor=forgetting_factor)\n",
    "        return Gs\n",
    "    \n",
    "    def bellman_q_by_v(self, s:int=0, a:int=0, forgetting_factor:float=1.0) -> float:\n",
    "        reward = self.Rsa[s][a]\n",
    "        v_next = 0\n",
    "        for next_s in range(len(self.Psas[s,a])):\n",
    "            if self.Psas[s,a,next_s] and next_s < len(self.v) - 1:\n",
    "                v_next += self.Psas[s,a,next_s] * self.v[next_s]     \n",
    "        Gs = reward + forgetting_factor * v_next\n",
    "        return Gs\n",
    "\n",
    "    def bellman_q(self, s:int=0, a:int=0, forgetting_factor:float=1.0) -> float:\n",
    "        reward = self.Rsa[s][a]\n",
    "        v_next = 0\n",
    "        for next_s in range(len(self.Psas[s,a])):\n",
    "            if self.Psas[s,a,next_s] and next_s < len(self.q) - 1:\n",
    "                v = 0\n",
    "                for next_a in range(len(self.policy[next_s])):\n",
    "                    v += self.policy[next_s][next_a] * self.q[next_s][next_a]\n",
    "                v_next += self.Psas[s,a,next_s] * v     \n",
    "        Gs = reward + forgetting_factor * v_next\n",
    "        return Gs\n",
    "    \n",
    "    def test(self, N_Iter:int=100):\n",
    "        print('### v(s)')\n",
    "        self.get_v(N_Iter)\n",
    "        print('### q(s,a)')\n",
    "        self.get_q(N_Iter)\n",
    "\n",
    "class MDP_PD(MDP):\n",
    "    #def __init__(self):\n",
    "    #    super().__init__()\n",
    "    \n",
    "    def set_policy_rsa_psas(self):\n",
    "        \"\"\"\n",
    "        PD Based Bellman Eq. with the above MDP\n",
    "        \"\"\"\n",
    "        S_df = pd.DataFrame({'S':[0,1,2,3,4]})\n",
    "        self.policy = pd.DataFrame({'s':[0,0, 1,1, 2,2, 3,3], 'a':[0,1, 0,1, 0,1, 0,1], 'pi':[1/2,1/2, 1/2,1/2, 1/2,1/2, 1/2,1/2]})\n",
    "        self.Rsa = pd.DataFrame({'s':[0,0, 1,1, 2,2, 3,3], 'a':[0,1, 0,1, 0,1, 0,1], 'R':[-1,0, -1,-2, 0,-2, 1,10]})\n",
    "        self.Psas = pd.DataFrame({'s':[0,0, 1,1, 2,2, 3,3,3,3], 'a':[0,1, 0,1, 0,1, 0,0,0,1], \n",
    "                                  'next_s':[0,1, 0,2, 4,3, 1,2,3,4], 'P':[1,1, 1,1, 1,1, 0.2,0.4,0.4,1.0]})\n",
    "        self.N_states = len(S_df)\n",
    "    \n",
    "    def init_policy_Rsa_Psas(self):\n",
    "        self.set_policy_rsa_psas()\n",
    "        self.N_actions_in_s = []\n",
    "        for s in range(self.N_states):\n",
    "            self.N_actions_in_s.append(len(self.policy.a[self.policy.s==s]))\n",
    "        \n",
    "    def bellman_v(self, s:int=0, forgetting_factor:float=1.0) -> float:\n",
    "        Gs = 0\n",
    "        policy_s = self.policy[self.policy.s == s]\n",
    "        for a in set(policy_s.a):\n",
    "            policy = policy_s.pi[policy_s.a == a].to_numpy()[0]\n",
    "            Gs += policy * self.bellman_q_by_v(\n",
    "                s=s, a=a, forgetting_factor=forgetting_factor)\n",
    "        # print('Gs =', Gs)\n",
    "        # print('policy =', policy)\n",
    "        return Gs\n",
    "    \n",
    "    def bellman_q_by_v(self, s:int=0, a:int=0, forgetting_factor:float=1.0) -> float:\n",
    "        Rsa = self.Rsa\n",
    "        Psas = self.Psas\n",
    "        reward = Rsa.R[(Rsa['s']==s) & (Rsa['a']==a)].to_numpy()[0]\n",
    "        v_next = 0\n",
    "        Psas_sa = Psas[(Psas.s==s) & (Psas.a==a)]\n",
    "        for next_s in set(Psas_sa.next_s):\n",
    "            if len(Psas_sa.P[Psas_sa.next_s==next_s]) and next_s < self.N_states - 1:\n",
    "                # print(Psas_sa.P[Psas_sa.next_s==next_s],Psas_sa.P[Psas_sa.next_s==next_s].to_numpy()[0])\n",
    "                v_next += Psas_sa.P[Psas_sa.next_s==next_s].to_numpy()[0] * self.v[next_s]     \n",
    "        Gs = reward + forgetting_factor * v_next\n",
    "        return Gs\n",
    "\n",
    "    def bellman_q(self, s:int=0, a:int=0, forgetting_factor:float=1.0) -> float:\n",
    "        Rsa = self.Rsa\n",
    "        Psas = self.Psas\n",
    "        reward = Rsa.R[(Rsa['s']==s) & (Rsa['a']==a)].to_numpy()[0]\n",
    "        v_next = 0\n",
    "        Psas_sa = Psas[(Psas.s==s) & (Psas.a==a)]\n",
    "        for next_s in set(Psas_sa.next_s):\n",
    "            if len(Psas_sa.P[Psas_sa.next_s==next_s]) and next_s < self.N_states - 1:\n",
    "                v = 0\n",
    "                policy_s = self.policy[self.policy.s == next_s]\n",
    "                for next_a in set(policy_s.a):\n",
    "                    policy = policy_s.pi[policy_s.a == next_a].to_numpy()[0]\n",
    "                    v += policy * self.q[next_s][next_a]\n",
    "                v_next += Psas_sa.P[Psas_sa.next_s==next_s].to_numpy()[0] * v     \n",
    "        Gs = reward + forgetting_factor * v_next\n",
    "        return Gs\n",
    "\n",
    "class MDP_PD_TTT(MDP_PD):\n",
    "    def set_policy_rsa_psas(self):\n",
    "        S_df = pd.DataFrame({'S':[0,1,2,3,4]})\n",
    "        self.policy = pd.DataFrame({'s':[0,0,0,1,2,3], 'a':[0,1,2,0,0,0], 'pi':[1/3, 1/3, 1/3, 1, 1, 1]})\n",
    "        self.Rsa = pd.DataFrame({'s':[0,0,0,1,2,3], 'a':[0,1,2,0,0,0], 'R':[1,0,-1/2,0,0,0]})\n",
    "        self.Psas = pd.DataFrame({'s':[0,0,0,0,0,1,2,3], 'a':[0,1,1,2,2,0,0,0], 'next_s':[4,1,2,3,4,4,4,4], 'P':[1,1/2,1/2,1/2,1/2,0,0,0]})\n",
    "        self.N_states = len(S_df)\n",
    "    \n",
    "    \n",
    "MDP().test()\n",
    "print('---------------------------------')\n",
    "MDP_PD().test()\n",
    "print('---------------------------------')\n",
    "MDP_PD_TTT().test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
